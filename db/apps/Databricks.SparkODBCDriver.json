{"appId":"w:Databricks.SparkODBCDriver","appShortcutName":"Simba Spark ODBC Driver","appDisplayName":"Simba Spark ODBC Driver","authorId":"w:winget","releaseTagName":"winget-2.9.2","downloadUrls":{"0":{"installerType":"WindowsInstallerMsi","asset":"","url":""},"1":{"installerType":"WindowsInstallerMsi","asset":"","url":""}},"install":{"win32":null,"winarm":null,"linux":null,"linuxArm64":null,"linuxArm7":null,"android":null},"displayImages":[],"description":"Simba Apache Spark ODBC Connector for direct SQL and HiveQL access to Apache Hadoop/Spark distributions.\n\nThe Simba Apache Spark ODBC Connector is used for direct SQL and HiveQL access to Apache Hadoop / Spark distributions, enabling Business Intelligence (BI), analytics, and reporting on Hadoop-based data. The connector efficiently transforms an applicationâ€™s SQL query into the equivalent form in HiveQL, which is a subset of SQL-92. If an application is Spark-aware, then the connector is configurable to pass the query through to the database for processing. The connector interrogates Spark to obtain schema information to present to a SQL-based application. Queries, including joins, are translated from SQL to HiveQL\nEnhancements & New Features\n\n * [SPARKO-1408] Process Name as Default UserAgentEntry\n\n   The driver will now use the process name as the UserAgentEntry if\n   the UserAgentEntry is not set.\n\n * [SPARKO-1472] Upgraded LTS support\n\n   The driver now uses LTS versions 11.3 through 15.4. For supported versions,\n   see the Installation and Configuration Guide.\n\n * [SPARKO-1482][05221097] Databricks domains support\n\n   The driver now supports cloud.databricks.us and cloud.databricks.mil\n   domains.\n\n * [SPARKO-1325][SPARKO-1435] Timestamp_NTZ support\n\n   The driver now recognizes timestamp_ntz columns in the following data\n   source tables:\n   - SQLGetTypeInfo (SQL_ALL_TYPES) now lists timestamp_ntz as an available\n   type.\n   - SQLGetTypeInfo (SQL_TIMESTAMP) now lists timestamp_ntz as one of the\n   types that map to SQL timestamps.\n   - SQLColumns now identifies timestamp_ntz columns correctly.\n   - If UseNativeQuery is set to 0, SQLColAttribute (SQL_DESC_TYPE_NAME)\n   identifies timestamp_ntz columns correctly.\n\n * [SPARKO-1384][SPARKO-1405][SPARKO-1411] CRL cache support\n\n   On Windows, the driver supports the CRL cache when UseSystemTruststore is\n   enabled (set to 1).\n\n * [SPARKO-1399][SPARKO-1417] Updated third-party libraries\n\n   The connector now uses the following third-party libraries:\n   - OpenSSL 3.0.16 (previously 3.0.15)\n   - libcURL 8.12.1 (previously 8.11.0)\n   - Expat 2.7.1 (previously 2.6.3)\n\n * [SPARKO-1404] VOID type support\n\n   The driver now supports VOID columns and lists them correctly in\n   SQLGetColumns calls.\n\n * [SPARKO-1419][SPARKO-1420] OAuth Token exchange support\n\n   The driver now supports OAuth Token exchange feature for IDP different\n   from the host. In these cases, OAuth access token (including BYOT) will be\n   exchanged for a Databricks in-house access token. For more information, see\n   the Installation and Configuration Guide.\n\n * [SPARKO-1474] Upgraded Windows Server support\n\n   The connector now supports Windows Server 2025. For a list of supported\n   versions, see the Installation and Configuration Guide.\n\nResolved Issues\nThe following issues have been resolved in Simba Apache Spark ODBC Connector\n2.9.2.\n\n * [SPARKO-1516][SPARKO-1518] A memory leak is resolved.\n\nKnown Issues\nThe following are known issues that you may encounter due to limitations in\nthe data source, the connector, or an application.\n\n * [SPARKO-1404] When querying tables that contain VOID columns, the server\n   returns an error.\n\n * [SPARKO-1101] When the Auth_AccessToken line length is longer than the\n   maximum limit of 1000, the connector returns an authentication error. For\n   more information, see the Installation and Configuration Guide.\n\n * [SPARKO-879] When connecting to a server that supports multiple catalogs,\n   the connector no longer reports the catalog for schemas and tables as\n   SPARK.\n\n   The Spark server now reports the catalog.\n\n * [SPARKO-670] In some cases, when retrieving timestamp data, the connector\n   returns an error.\n\n   In some cases, when connecting to certain distributions of\n   Apache Spark, the connector returns the following error: \"Conversion from\n   number to string failed due to undersized character buffer\". This issue\n   affects versions 2.6.12 to 2.6.14 of the Spark ODBC connector.\n\n   As a workaround, set EnableArrow=0 in the connection string or DSN.\n\n * [SPARKO-620] Issue with date and timestamp before the beginning of the\n   Gregorian calendar when connecting to Spark 2.4.4 or later, or versions\n   previous to 3.0, with Arrow result set serialization.\n\n   When using Spark 2.4.4 or later, or versions previous to Spark 3.0, DATE\n   and TIMESTAMP data before October 15, 1582 may be returned incorrectly if\n   the server supports serializing query results using Apache Arrow. This\n   issue should not impact most distributions of Apache Spark.\n\n   To confirm if your distribution of Spark 2.4.4 or later has been impacted\n   by this issue, you can execute the following query:\n\n   SELECT DATE '1581-10-14'\n\n   If the result returned by the connector is 1581-10-24, then you are\n   impacted by the issue. In this case, if your data set contains date and/or\n   timestamp data earlier than October 15, 1582, you can work around this\n   issue by adding EnableArrow=0 in your DSN or connection string to disable\n   the Arrow result set serialization feature.\n\n * When retrieving data from a BINARY column, a ClassCastException error\n   occurs.\n\n   In Spark 1.6.3 or earlier, the server sometimes returns a\n   ClassCastException error when attempting to retrieve data from a BINARY\n   column.\n\n   This issue is fixed as of Spark 2.0.0.\n\n   For more information, see the JIRA issue posted by Apache named \"When\n   column type is binary, select occurs ClassCastException in Beeline\" at\n   https://issues.apache.org/jira/browse/SPARK-12143.","repo":{"author":"microsoft","repo":"winget-pkgs"},"usrVersion":"2.9.2","version":0,"site":"https://www.databricks.com/","source":"Simba Technologies Inc.","license_or_tos":"Proprietary","resources":null,"verified":false}