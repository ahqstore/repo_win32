{"appId":"winget_app_Ollama.Ollama","appShortcutName":"Winget Application","appDisplayName":"Ollama","authorId":"winget","releaseTagName":"winget-0.5.5","downloadUrls":{"0":{"installerType":"WindowsInstallerMsi","asset":"","url":""},"1":{"installerType":"WindowsInstallerExe","asset":"","url":"https://github.com/ollama/ollama/releases/download/v0.5.5/OllamaSetup.exe"}},"install":{"win32":{"assetId":1,"exec":null,"scope":"User","installerArgs":null},"winarm":null,"linux":null,"linuxArm64":null,"linuxArm7":null,"android":null},"displayImages":[],"description":"Get up and running with large language models locally.\n\n\nNew models\n- Phi-4: Phi 4 is a 14B parameter, state-of-the-art open model from Microsoft.\n- Command R7B: the smallest model in Cohere's R series delivers top-tier speed, efficiency, and quality to build powerful AI applications on commodity GPUs and edge devices.\n- DeepSeek-V3: A strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.\n- OLMo 2: a new family of 7B and 13B models trained on up to 5T tokens. These models are on par with or better than equivalently sized fully open models, and competitive with open-weight models such as Llama 3.1 on English academic benchmarks.\n- Dolphin 3: the next generation of the Dolphin series of instruct-tuned models designed to be the ultimate general purpose local model, enabling coding, math, agentic, function calling, and general use cases.\n- SmallThinker: A new small reasoning model fine-tuned from the Qwen 2.5 3B Instruct model.\n- Granite 3.1 Dense: 2B and 8B text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated significant improvements over their predecessors in performance and speed in IBMâ€™s initial testing.\n- Granite 3.1 MoE: 1B and 3B long-context mixture of experts (MoE) Granite models from IBM designed for low latency usage.\nWhat's Changed\n- The /api/create API endpoint that powers ollama create has been changed to improve conversion time and also accept a JSON object. Note: this change is not backwards compatible. If importing models, make sure you're using version 0.5.5 or later for both Ollama and the ollama CLI when running ollama create. If using ollama.create in the Python or JavaScript libraries, make sure to update to the latest version.\n- Fixed runtime error that would occur when filling the model's context window\n- Fixed crash that would occur when quotes were used in /save\n- Fixed errors that would occur when sending x-stainless headers from OpenAI clients\nNew Contributors\n- @Squishedmac made their first contribution in https://github.com/ollama/ollama/pull/8172\n- @erusev made their first contribution in https://github.com/ollama/ollama/pull/7950\n- @olumolu made their first contribution in https://github.com/ollama/ollama/pull/8227\n- @paradoxical-dev made their first contribution in https://github.com/ollama/ollama/pull/8242\n- @belfie13 made their first contribution in https://github.com/ollama/ollama/pull/8215\n- @Docteur-RS made their first contribution in https://github.com/ollama/ollama/pull/7259\n- @anxkhn made their first contribution in https://github.com/ollama/ollama/pull/8082\n- @ubaldus made their first contribution in https://github.com/ollama/ollama/pull/8307\n- @isamu made their first contribution in https://github.com/ollama/ollama/pull/8343\nFull Changelog: https://github.com/ollama/ollama/compare/v0.5.4...v0.5.5","repo":{"author":"microsoft","repo":"winget-pkgs"},"version":"0.5.5","site":"https://ollama.com/","source":"Ollama","license_or_tos":"MIT","resources":null,"verified":false}