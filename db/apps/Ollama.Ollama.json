{"appId":"w:Ollama.Ollama","appShortcutName":"Ollama","appDisplayName":"Ollama","authorId":"w:winget","releaseTagName":"winget-0.12.8","downloadUrls":{"1":{"installerType":"WindowsInstallerExe","asset":"","url":"https://github.com/ollama/ollama/releases/download/v0.12.8/OllamaSetup.exe"},"0":{"installerType":"WindowsInstallerMsi","asset":"","url":""}},"install":{"win32":{"assetId":1,"exec":null,"scope":"User","installerArgs":null},"winarm":null,"linux":null,"linuxArm64":null,"linuxArm7":null,"android":null},"displayImages":[],"description":"Get up and running with large language models locally.\n\n\nWhat's Changed\n- qwen3-vl performance improvements, including flash attention support by default\n- qwen3-vl will now output less leading whitespace in the response when thinking\n- Fixed issue where deepseek-v3.1 thinking could not be disabled in Ollama's new app\n- Fixed issue where qwen3-vl would fail to interpret images with transparent backgrounds\n- Ollama will now stop running a model before removing it via ollama rm\n- Fixed issue where prompt processing would be slower on Ollama's engine\n- Ignore unsupported iGPUs when doing device discovery on Windows\nNew Contributors\n- @athshh made their first contribution in https://github.com/ollama/ollama/pull/12822\nFull Changelog: https://github.com/ollama/ollama/compare/v0.12.7...v0.12.8","repo":{"author":"microsoft","repo":"winget-pkgs"},"usrVersion":"0.12.8","version":0,"site":"https://ollama.com/","source":"Ollama","license_or_tos":"MIT","resources":null,"verified":false}