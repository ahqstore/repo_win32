{"appId":"w:Ollama.Ollama","appShortcutName":"Ollama","appDisplayName":"Ollama","authorId":"w:winget","releaseTagName":"winget-0.14.3","downloadUrls":{"1":{"installerType":"WindowsInstallerExe","asset":"","url":"https://github.com/ollama/ollama/releases/download/v0.14.3/OllamaSetup.exe"},"0":{"installerType":"WindowsInstallerMsi","asset":"","url":""}},"install":{"win32":{"assetId":1,"exec":null,"scope":"User","installerArgs":null},"winarm":null,"linux":null,"linuxArm64":null,"linuxArm7":null,"android":null},"displayImages":[],"description":"Get up and running with large language models locally.\n\n\nNew models\n- GLM-4.7-Flash: As the strongest model in the 30B class, GLM-4.7-Flash offers a new option for lightweight deployment that balances performance and efficiency.\n- LFM2.5-1.2B-Thinking: LFM2.5 is a new family of hybrid models designed for on-device deployment.\nWhat's Changed\n- Fixed issue where Ollama's macOS app would interrupt system shutdown\n- Fixed ollama create and ollama show commands for experimental models\n- The /api/generate API can now be used for image generation\n- Fixed minor issues in Nemotron-3-Nano tool parsing\n- Fixed issue where removing an image generation model would cause it to first load\n- Fixed issue where ollama rm would only stop the first model in the list if it were running\nFull Changelog: https://github.com/ollama/ollama/compare/v0.14.2...v0.14.3-rc2","repo":{"author":"microsoft","repo":"winget-pkgs"},"usrVersion":"0.14.3","version":0,"site":"https://ollama.com/","source":"Ollama","license_or_tos":"MIT","resources":null,"verified":false}