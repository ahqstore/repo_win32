{"appId":"w:Ollama.Ollama","appShortcutName":"Ollama","appDisplayName":"Ollama","authorId":"w:winget","releaseTagName":"winget-0.13.1","downloadUrls":{"1":{"installerType":"WindowsInstallerExe","asset":"","url":"https://github.com/ollama/ollama/releases/download/v0.13.1/OllamaSetup.exe"},"0":{"installerType":"WindowsInstallerMsi","asset":"","url":""}},"install":{"win32":{"assetId":1,"exec":null,"scope":"User","installerArgs":null},"winarm":null,"linux":null,"linuxArm64":null,"linuxArm7":null,"android":null},"displayImages":[],"description":"Get up and running with large language models locally.\n\n\nNew models\n- Ministral-3: The Ministral 3 family is designed for edge deployment, capable of running on a wide range of hardware.\nWhat's Changed\n- nomic-embed-text will now use Ollama's engine by default\n- Tool calling support for cogito-v2.1\n- Fixed issues with CUDA VRAM discovery\n- Fixed link to docs in Ollama's app\nFull Changelog: https://github.com/ollama/ollama/compare/v0.13.0...v0.13.1-rc2","repo":{"author":"microsoft","repo":"winget-pkgs"},"usrVersion":"0.13.1","version":0,"site":"https://ollama.com/","source":"Ollama","license_or_tos":"MIT","resources":null,"verified":false}