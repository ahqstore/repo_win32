{"appId":"w:Ollama.Ollama","appShortcutName":"Ollama","appDisplayName":"Ollama","authorId":"w:winget","releaseTagName":"winget-0.12.10","downloadUrls":{"0":{"installerType":"WindowsInstallerMsi","asset":"","url":""},"1":{"installerType":"WindowsInstallerExe","asset":"","url":"https://github.com/ollama/ollama/releases/download/v0.12.10/OllamaSetup.exe"}},"install":{"win32":{"assetId":1,"exec":null,"scope":"User","installerArgs":null},"winarm":null,"linux":null,"linuxArm64":null,"linuxArm7":null,"android":null},"displayImages":[],"description":"Get up and running with large language models locally.\n\n\nollama run now works with embedding models\nollama run can now run embedding models to generate vector embeddings from text:\nollama run embeddinggemma \"Hello world\"\nContent can also be provided to ollama run via standard input:\necho \"Hello world\" | ollama run embeddinggemma\nWhat's Changed\n- Fixed errors when running qwen3-vl:235b and qwen3-vl:235b-instruct\n- Enable flash attention for Vulkan (currently needs to be built from source)\n- Add Vulkan memory detection for Intel GPU using DXGI+PDH\n- Ollama will now return tool call IDs from the /api/chat API\n- Fixed hanging due to CPU discovery\n- Ollama will now show login instructions when switching to a cloud model in interactive mode\n- Fix reading stale VRAM data\n- ollama run now works with embedding models\nNew Contributors\n- @ryanycoleman made their first contribution in https://github.com/ollama/ollama/pull/11740\n- @Rajathbail made their first contribution in https://github.com/ollama/ollama/pull/12929\n- @virajwad made their first contribution in https://github.com/ollama/ollama/pull/12664\n- @AXYZdong made their first contribution in https://github.com/ollama/ollama/pull/8601\nFull Changelog: https://github.com/ollama/ollama/compare/v0.12.9...v0.12.10","repo":{"author":"microsoft","repo":"winget-pkgs"},"usrVersion":"0.12.10","version":0,"site":"https://ollama.com/","source":"Ollama","license_or_tos":"MIT","resources":null,"verified":false}