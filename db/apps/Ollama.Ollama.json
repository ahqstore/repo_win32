{"appId":"w:Ollama.Ollama","appShortcutName":"Ollama","appDisplayName":"Ollama","authorId":"w:winget","releaseTagName":"winget-0.16.0","downloadUrls":{"0":{"installerType":"WindowsInstallerMsi","asset":"","url":""},"1":{"installerType":"WindowsInstallerExe","asset":"","url":"https://github.com/ollama/ollama/releases/download/v0.16.0/OllamaSetup.exe"}},"install":{"win32":{"assetId":1,"exec":null,"scope":"User","installerArgs":null},"winarm":null,"linux":null,"linuxArm64":null,"linuxArm7":null,"android":null},"displayImages":[],"description":"Get up and running with large language models locally.\n\n\nNew models\n- GLM-5: A strong reasoning and agentic model from Z.ai with 744B total parameters (40B active), built for complex systems engineering and long-horizon tasks.\n- MiniMax-M2.5: a new state-of-the-art large language model designed for real-world productivity and coding tasks.\nNew ollama\nThe new ollama command makes it easy to launch your favorite apps with models using Ollama\nOllama screenshot 2026-02-12 at 04 48 55@2x\nWhat's Changed\n- Launch Pi with ollama launch pi\n- Improvements to Ollama's MLX runner to support GLM-4.7-Flash\n- Ctrl+G will now allow for editing text prompts when running a model\nFull Changelog: v0.15.6...v0.16.0","repo":{"author":"microsoft","repo":"winget-pkgs"},"usrVersion":"0.16.0","version":0,"site":"https://ollama.com/","source":"Ollama","license_or_tos":"MIT","resources":null,"verified":false}