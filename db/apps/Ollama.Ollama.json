{"appId":"w:Ollama.Ollama","appShortcutName":"Ollama","appDisplayName":"Ollama","authorId":"w:winget","releaseTagName":"winget-0.14.0","downloadUrls":{"1":{"installerType":"WindowsInstallerExe","asset":"","url":"https://github.com/ollama/ollama/releases/download/v0.14.0/OllamaSetup.exe"},"0":{"installerType":"WindowsInstallerMsi","asset":"","url":""}},"install":{"win32":{"assetId":1,"exec":null,"scope":"User","installerArgs":null},"winarm":null,"linux":null,"linuxArm64":null,"linuxArm7":null,"android":null},"displayImages":[],"description":"Get up and running with large language models locally.\n\n\nWhat's Changed\n- ollama run --experimental CLI will now open a new Ollama CLI that includes an agent loop and the bash tool\n- Anthropic API compatibility: support for the /v1/messages API\n- A new REQUIRES command for the Modelfile allows declaring which version of Ollama is required for the model\n- For older models, Ollama will avoid an integer underflow on low VRAM systems during memory estimation\n- More accurate VRAM measurements for AMD iGPUs\n- Ollama's app will now highlight swift soure code\n- An error will now return when embeddings return NaN or -Inf\n- Ollama's Linux install bundles files now use zst compression\n- New experimental support for image generation models, powered by MLX\nNew Contributors\n- @Vallabh-1504 made their first contribution in https://github.com/ollama/ollama/pull/13550\n- @majiayu000 made their first contribution in https://github.com/ollama/ollama/pull/13596\n- @harrykiselev made their first contribution in https://github.com/ollama/ollama/pull/13615\nFull Changelog: https://github.com/ollama/ollama/compare/v0.13.5...v0.14.0-rc2","repo":{"author":"microsoft","repo":"winget-pkgs"},"usrVersion":"0.14.0","version":0,"site":"https://ollama.com/","source":"Ollama","license_or_tos":"MIT","resources":null,"verified":false}