{"appId":"w:Ollama.Ollama","appShortcutName":"Ollama","appDisplayName":"Ollama","authorId":"w:winget","releaseTagName":"winget-0.12.0","downloadUrls":{"0":{"installerType":"WindowsInstallerMsi","asset":"","url":""},"1":{"installerType":"WindowsInstallerExe","asset":"","url":"https://github.com/ollama/ollama/releases/download/v0.12.0/OllamaSetup.exe"}},"install":{"win32":{"assetId":1,"exec":null,"scope":"User","installerArgs":null},"winarm":null,"linux":null,"linuxArm64":null,"linuxArm7":null,"android":null},"displayImages":[],"description":"Get up and running with large language models locally.\n\n\nWhat's Changed\n- Models with the Bert architecture now run on Ollama's engine\n- Models with the Qwen 3 architecture now run on Ollama's engine\n- Fix issue where older NVIDIA GPUs would not be detected if newer drivers were installed\n- qwen3-coder now supports tools\n- Fixed issue where models would not be imported correctly with ollama create\n- Ollama will skip parsing the initial <think> if provided in the prompt for /api/generate by @rick-github\nNew Contributors\n- @egyptianbman made their first contribution in https://github.com/ollama/ollama/pull/12300\n- @russcoss made their first contribution in https://github.com/ollama/ollama/pull/12280\nFull Changelog: https://github.com/ollama/ollama/compare/v0.11.11...v0.12.0-rc0","repo":{"author":"microsoft","repo":"winget-pkgs"},"version":"0.12.0","site":"https://ollama.com/","source":"Ollama","license_or_tos":"MIT","resources":null,"verified":false}