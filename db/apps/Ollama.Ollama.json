{"appId":"w:Ollama.Ollama","appShortcutName":"Ollama","appDisplayName":"Ollama","authorId":"w:winget","releaseTagName":"winget-0.13.2","downloadUrls":{"0":{"installerType":"WindowsInstallerMsi","asset":"","url":""},"1":{"installerType":"WindowsInstallerExe","asset":"","url":"https://github.com/ollama/ollama/releases/download/v0.13.2/OllamaSetup.exe"}},"install":{"win32":{"assetId":1,"exec":null,"scope":"User","installerArgs":null},"winarm":null,"linux":null,"linuxArm64":null,"linuxArm7":null,"android":null},"displayImages":[],"description":"Get up and running with large language models locally.\n\n\nWhat's Changed\n- Flash attention is now enabled by default for vision models such as mistral-3, gemma3, qwen3-vl and more. This improves memory utilization and performance when providing images as input.\n- Fixed GPU detection on multi-GPU CUDA machines\n- Fixed issue where deepseek-v3.1 would always think even with thinking is disabled in Ollama's app\nNew Contributors\n- @chengcheng84 made their first contribution in https://github.com/ollama/ollama/pull/13265\n- @nathan-hook made their first contribution in https://github.com/ollama/ollama/pull/13256\nFull Changelog: https://github.com/ollama/ollama/compare/v0.13.1...v0.13.2-rc0","repo":{"author":"microsoft","repo":"winget-pkgs"},"usrVersion":"0.13.2","version":0,"site":"https://ollama.com/","source":"Ollama","license_or_tos":"MIT","resources":null,"verified":false}