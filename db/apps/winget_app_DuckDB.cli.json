{"appId":"winget_app_DuckDB.cli","appShortcutName":"Winget Application","appDisplayName":"DuckDB CLI","authorId":"winget","releaseTagName":"winget-1.2.0","downloadUrls":{"0":{"installerType":"WindowsInstallerMsi","asset":"","url":""},"1":{"installerType":"WindowsInstallerMsi","asset":"","url":""}},"install":{"win32":null,"winarm":null,"linux":null,"linuxArm64":null,"linuxArm7":null,"android":null},"displayImages":[],"description":"DuckDB is an in-process SQL OLAP Database Management System\n\nDuckDB is a high-performance analytical database system.\nIt is designed to be fast, reliable and easy to use.\nDuckDB provides a rich SQL dialect, with support far beyond basic SQL.\nThis release of DuckDB is named \"Histrionicus\" after the good-looking Harlequin duck (Histrionicus Histrionicus) that inhabits \"cold fast moving streams in North America, Greenland, Iceland and eastern Russia\".\nPlease also refer to the announcement blog post: https://duckdb.org/2025/02/05/announcing-duckdb-120\nWhat's Changed\n- Optimise division by a constant at runtime for integer division by @JAicewizard in #10348\n- Add cross join to Python Relational and PySpark API by @khalidmammadov in #13519\n- Fix #13805: throw a more descriptive error message when an on-disk file is referenced using a replacement scan for an unsupported file format by @Mytherin in #13871\n- Make sampling accept parameters at the parser/transformer layer by @Mytherin in #13903\n- Fix #13867: use 64-bit random numbers to generate random numbers for random() by @Mytherin in #13920\n- Fix #13769: when binding views, always first search in the schema that the view is defined in by @Mytherin in #13921\n- Rework table bindings to be components (catalog, schema, table) instead of flat strings by @Mytherin in #14017\n- Add auto-loadable extension settings to duckdb_config_count and duckdb_get_config_flag by @Mytherin in #14021\n- Fix #10961 - in the HAVING clause - in case of column name conflicts, bind to aliases instead of to ungrouped columns by @Mytherin in #14023\n- Enable filter pushdown through Logical Unnest by @Tmonster in #14008\n- Allow duplicate table aliases in the table binder by @Mytherin in #14035\n- Unify DESCRIBE [query] and DESCRIBE [table] by @Mytherin in #14039\n- Support qualified identifiers in the EXCLUDE clause by @Mytherin in #14043\n- Add SMALLER_BINARY flag to reduce binary size by @Mytherin in #14057\n- Smaller Binary: remove more templates from arg_min_max by @Mytherin in #14071\n- Unify entropy and mode aggregates - and skip specialized implementations for entropy with smaller binary by @Mytherin in #14080\n- [Python] Add set_default_connection to the duckdb module by @Tishj in #13442\n- Provide workaround for prefetching parquet files with incorrect page offsets by @samansmink in #13697\n- Move core_functions to a separate extension by @Mytherin in #14149\n- PySpark df.drop() to support expressions by @khalidmammadov in #14059\n- add some RealNest benchmarks by @hmeriann in #13345\n- feed table function into multifilereader initialization by @samansmink in #14112\n- [Dev] Fix an issue causing ExecuteTask to do much more work than intended by @Tishj in #14034\n- Overhaul Parquet dictionary handling by @hannes in #14194\n- [Feature] Allow passing the catalog (database name) to appender by @taniabogatsch in #13692\n- Add Taxi Dataset Benchmark by @pdet in #14197\n- Feature #3036: Window Spooling by @hawkfish in #14181\n- Small C Extension API changes by @samansmink in #13987\n- Add HTML and Graphviz support for explain analyze by @abramk in #13942\n- Fix #13064: offer more suggestions with same score by @Damon07 in #14048\n- New Algorithm to find a new line on parallel execution by @pdet in #14260\n- Making client context lock optional for relation binding by @pdet in #14093\n- [Feature] Allow passing the catalog during C API appender creation by @taniabogatsch in #14256\n- Make test random output ordered by @Damon07 in #14267\n- Skip test_window_distinct by @Mytherin in #14309\n- Taxi Benchmark by @pdet in #14301\n- Switch to shared pointer for multfilelists by @samansmink in #14291\n- Push #14298 to feature branch by @flashmouse in #14311\n- Implement PullUp Empty Results optimizer by @Tmonster in #13524\n- [Export/Import] Use the DependencyManager to (stable) sort the entries before export by @Tishj in #14196\n- Partitioning-Aware Aggregation and Partitioning-Aware Infrastructure by @Mytherin in #14329\n- Add df.unionByName to PySpark API by @khalidmammadov in #14063\n- Or filter pushdown into zone maps by @Tmonster in #14313\n- Get the current setting in the database file opener by @Mytherin in #14361\n- [Feature + Fix] Support ALTER TABLE tbl ALTER col TYPE USING and fix null handling in struct_insert by @taniabogatsch in #14359\n- [C API] Add table_description_create_ext and table_description_get_column_name by @taniabogatsch in #14285\n- Move _rtools platform to be equivalent to _mingw by @carlopi in #14368\n- Fix for accidental like skip in the CSV Buffer by @pdet in #14380\n- Table locks - always grab table locks through the transaction interface by @Mytherin in #14379\n- Implementing array_slice and [] for BLOB by @hannes in #14358\n- Rework settings handling and implement auto-generation for new ones by @Mytherin in #14383\n- Rework settings handling and implement auto-generation for new ones by @chrisiou in #14018\n- Arrow list buffer - suggest setting arrow_large_buffer_size to true when regular list buffer size is exceeded by @Mytherin in #14384\n- Fix incorrect merge conflict resolution in workflow file by @Mytherin in #14390\n- Update Parquet Thrift to latest version by @hannes in #14258\n- Reformat list functions by @c-herrewijn in #14372\n- Tidy Check to do complete run also on feature by @carlopi in #14394\n- [Python] Use an ArrowQueryResult in FetchArrowTable when possible. by @Tishj in #14319\n- Make mysql_scanner auto-loadable, and add mysql/postgres secrets by @Mytherin in #14392\n- Improvement the speed of table sample systems by @continue-revolution in #12631\n- Support defining column names in CTAS by @douenergy in #14327\n- Fix pointer indirection in pyrelation.cpp by @carlopi in #14403\n- Fix idx_t to int64_t implicit conversion flagged by clang-tidy by @carlopi in #14402\n- Storage: make ROW_GROUP_SIZE configurable by @Mytherin in #14406\n- [Dev] Update vendored ZSTD to v1.5.6 by @Tishj in #14360\n- Top-N: Rework to use heap of sort keys by @Mytherin in #14424\n- reformat string functions by @c-herrewijn in #14400\n- Prefix Aliases in SQL by @hannes in #14436\n- [Dev] Optimize ValidityMask when reading from a ColumnDataCollection by @Tishj in #14416\n- [Dev] Further optimize the CDC ValidityMask deserialization by @Tishj in #14448\n- Reformat date and map functions by @c-herrewijn in #14425\n- Reformat generic functions by @c-herrewijn in #14423\n- Push dynamically generated join filters through UNION, UNNEST and AGGREGATE by @Mytherin in #14453\n- Try auto-casting for mismatching data chunks in the Appender API by @taniabogatsch in #14433\n- Implement DELTA_BINARY_PACKED compression in Parquet writer by @lnkuiper in #14257\n- Eviction Queue Partitioning by @lnkuiper in #14375\n- Implement map_extract_first by @lnkuiper in #14175\n- RowGroup no longer lives in format namespace by @Mytherin in #14469\n- Convert the shell from C to C++ by @Mytherin in #14473\n- Fixing an issue with parquet dictionary reading by @hannes in #14438\n- Strip down unused/unsupported options from the CLI by @Mytherin in #14478\n- [PySpark] Add withColumns, withColumnsRenamed, cos, acos, any_value, approx_count_distinct and various array functions by @binste in #14347\n- CLI Code Cleanup: move all shell functions into the ShellState by @Mytherin in #14483\n- CLI Code Cleanup: Move rendering logic into separate Renderer classes by @Mytherin in #14485\n- Reformat compressed materialization functions by @c-herrewijn in #14470\n- Internal #3273: Shared Window Expressions by @hawkfish in #14450\n- CLI Code Cleanup: rework metadata commands in the shell by @Mytherin in #14503\n- CSV Parallel Reading Validation by @pdet in #14439\n- Avoid recompilations of duckdb when there are no actual changes by @carlopi in #14176\n- Add -safe mode to shell which disables external access, and remove SQLite UDFs from the shell by @Mytherin in #14509\n- [PySpark] Add functions covar_pop, covar_samp, call_functions, endswith, startswith, exp, factorial, log2, ln, degrees, radians, atan, atan2, tan, round, bround by @binste in #14454\n- Reformat arithmetic operators by @c-herrewijn in #14489\n- add attach with default tables by @samansmink in #14118\n- Add duckdb_param_logical_type by @Giorgi in #14515\n- Remove most BUILD_ options for extensions, using CORE_EXTENSIONS by @carlopi in #14531\n- CLI: more code clean-up by @Mytherin in #14551\n- Reformat nested and sequence functions by @c-herrewijn in #14495\n- Parquet: Fixing selection vector calculation by @hannes in #14558\n- CLI: Fix for .mode markdown rendering after refactor by @Mytherin in #14569\n- Out-Of-Core Updates & Deletes by @Mytherin in #14559\n- Manage enable_external_access at the FileSystem level, and add allowed_paths and allowed_directories option by @Mytherin in #14568\n- feat(iejoin): use sort to replace binary search in iejoin by @my-vegetable-has-exploded in #14507\n- Clean-up distinct statistics - add hashes cache add the Append and Vacuum layers, and remove unnecessary lock by @Mytherin in #14578\n- [PySpark] Test Spark API with actual PySpark as backend by @binste in #14526\n- Internal #3273: Shared Window Frames by @hawkfish in #14544\n- Reformat aggregate functions by @c-herrewijn in #14530\n- Expose threshold argument of Jaro-Winkler similarity by @zmbc in #12079\n- No pushing filters below projections that cast to a lower logical type id by @Tmonster in #13617\n- Implement left_projection_map for joins by @lnkuiper in #13729\n- remove superfluous comment by @c-herrewijn in #14586\n- [Dev] Make the regression_test_runner easier to replicate by @Tishj in #14557\n- [PySpark] Add dataframe methods drop_duplicates, intersectAll, exceptAll, toArrow by @binste in #14458\n- Internal #3381: Window Race Condition by @hawkfish in #14599\n- Rework generated EnumUtil code by @Mytherin in #14391\n- Force aggregate state to be trivially_destructible, unless AggregateDestructorType::LEGACY is used by @Mytherin in #14615\n- AWS - remove expected error message by @Mytherin in #14633\n- Temp directory compression by @lnkuiper in #14465\n- Add support for SELECT * RENAME by @Mytherin in #14650\n- [PySpark] Add autocompletion for column names to dataframes by @binste in #14577\n- Force aggregate state to be is_trivially_move_constructible by @lnkuiper in #14640","repo":{"author":"microsoft","repo":"winget-pkgs"},"version":"1.2.0","site":"https://www.duckdb.org/","source":"DuckDB","license_or_tos":"MIT","resources":null,"verified":false}